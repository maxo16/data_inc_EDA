---
title: "Capstone Proposal"
author: "Max O'Krepki"
date: "2/22/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

At this point, I should focus on making this more flexible. Just go ahead and cutout the population stuff, it's just fluff that isn't needed right now. Should probably get rid of a bunch of other fluff as well. 

```{r}
library(data.table)
library(dplyr)
library(treemap)
library(sf)
library(readr)
library(leaflet)
library(lubridate)
library(htmltools)
library(ggplot2)
library(ggthemes)
library(tidyverse)
```

I'll just leave it as fread for now but in the future I'll probably keep using the tidyverse read_csv. I'm just not really seeing any performance gains here. 
```{r}
sf_311 <- fread("/Users/max/Dropbox/Data Incubator/311_Cases.csv")
sf_311 <- sf_311[,1:20]
```


```{r}
#Let me see if I can't get it to work with the dates as is. 
mdy_hms(sf_311$Opened[1:10], tz = "US/Pacific")

sf_311$Opened <- mdy_hms(sf_311$Opened, tz = "US/Pacific")
sf_311$Closed <- mdy_hms(sf_311$Closed, tz = "US/Pacific")

# Looks like the tidyverse solutions might be slower, sometimes, they're definitely faster at other times. However, I think the tradeoff in the easier syntax is worth it, though. 
sf_311 <- sf_311 %>% drop_na(Closed)

# Filter out coordinates that are clearly wrong. 
sf_311 <- sf_311 %>% filter(Longitude >= -123 & Longitude <= -122 & Latitude >= 37.7 & Latitude <= 38)
```


```{r}
# Lesson learned is, when doing something, search for a tidyverse solution. 
summary_by_category <- sf_311 %>% group_by(Category) %>% summarize("category_summary" = n())

summary_by_category <- summary_by_category %>% arrange(-category_summary)

summary_by_category <- summary_by_category %>% mutate(pct_of_total = (category_summary/sum(category_summary))*100, cumpct = cumsum(pct_of_total))

```

Let me just start off with a tree map then go from there. 
```{r}
# Tree map of all
treemap(summary_by_category, index = "Category", vSize = "pct_of_total", type = "index", title = "Percent of Total Calls")

# Tree map of top 10
treemap(summary_by_category[1:10,], index = "Category", vSize = "pct_of_total", type = "index", title = "Percent of Total Calls, Top 10")
```

```{r}
sf_311 <- sf_311 %>% filter(Category %in% summary_by_category$Category[1:4])
```

Create the category of interest variable here. 
```{r}
kitty <- summary_by_category$Category[2]

# Creates the filtered data frame. 
sf_311_kitty <- sf_311 %>% filter(Category == kitty)
```


Create an economist style histogram. I'm not filtering the data frame that's going in, I'm just manually cutting it off in the plotting part. 
```{r}
# The "math" for the histogram
# This orders the data frame by date.
sf_311_kitty <- arrange(sf_311_kitty, Opened)

# Create a lag variable between calls. That's the next thing I'm interested in. 
sf_311_kitty <- sf_311_kitty %>% mutate(time_diff = Opened - lag(Opened), diff_secs = as.numeric(time_diff, units = 'secs'))


# This is just a good column to have, makes sorting based on statistical criteria a little easier.
sf_311_kitty <- sf_311_kitty %>% mutate(diff_z_score = scale(diff_secs))


# Code that creates the actual histogram. 
barfill <- "#4271AE"
barlines <- "#1F3552"

p7 <- ggplot(sf_311_kitty, aes(x = diff_secs)) +
        geom_histogram(aes(y = ..count..), binwidth = 60,
                   colour = barlines, fill = barfill) +
        scale_x_continuous(name = "Time between service call",
                              breaks = seq(0, 1200, 120),
                              limits=c(0, 1200)) +
        scale_y_continuous(name = "Count", labels = scales::comma) +
        ggtitle("Frequency histogram of time \nbetween service calls") +
        theme_economist() +
        theme(legend.position = "bottom", legend.direction = "horizontal",
              legend.box = "horizontal",
              legend.key.size = unit(1, "cm"),
              plot.title = element_text(family="Tahoma"),
              text = element_text(family = "Tahoma", hjust = 0.5),
              axis.title = element_text(size = 12),
              legend.text = element_text(size = 9),
              legend.title=element_text(face = "bold", size = 9))
p7
```


# Geographic hotspots. 
Data's loaded. May have to move this up since I'm using it for the histogram as well. 
So it was just the one line I needed to move. So things look good, however, I'm realizing that when using the population data, it filtered to only the city. Figure out a way to bring that back in. 
```{r}
ba_bgs <- st_read("/Users/max/Dropbox/Stanford Stuff/Classes/PUBLPOL/Y23 Spring 2020/Thesis/Data/sf_bay_clipped/2e262912-207a-4424-9e17-08041688d59c202044-1-130emnv.fypm.shp")

population_data_bgs <- read_csv("~/Dropbox/Stanford Stuff/Classes/PUBLPOL/Y23 Spring 2020/Thesis/Data/population_data_bgs.csv")

# Don't think I need any of this now because it was for the population data. Just comment out and see if the assets are recreated. 

ba_bgs <- ba_bgs[,c(6,10)] #Don't really need the id's but may as well keep them so I have a unique identifier for each row. 
ba_bgs <- ba_bgs %>% mutate(geoid_long = paste("1500000US", blkgrpid, sep = ""))

ba_bgs <- ba_bgs[,c(2,3)]
# This is where it filters to just block groups in SF, I'll just keep this and rename things accordingly. 
ba_bgs <- ba_bgs %>% filter(geoid_long %in% population_data_bgs$id)
```

Move creation of sf_311_kitty here. 
```{r}
# Making the sf object from the data.table
sf_311_kitty <- st_as_sf(sf_311_kitty, coords = c("Longitude", "Latitude"), crs = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

# Doing the spatial join to get block group data to each call
sf_311_kitty <- st_join(sf_311_kitty, ba_bgs["geoid_long"])
```


I'll just start with the street and sidewalk cleaning category. Anything I do there abstracts to other categories. 
```{r}
# Would probably be good to keep in mind for further analysis. 
sf_311_kitty %>% group_by(`Request Details`) %>% summarize(n())
```

Group calls by geoid. Okay, not sure what happened the first time but it appears to have worked this time. 

This summary data frame is the new data frame that contains the data that will eventually get mapped. 
```{r}
sf_311_kitty_summary <- sf_311_kitty %>% group_by(geoid_long) %>% summarize("calls_bg" = n())

# Join it back to the bgs then plot. Have to join it to the sf object,  
sf_311_kitty_summary <- ba_bgs %>% left_join(as.data.frame(sf_311_kitty_summary)[,1:2], by = "geoid_long")

# This did the trick. 
sf_311_kitty_summary <-  sf_311_kitty_summary %>% drop_na(calls_bg)
```


Let me try a discrete color scheme but with lots of buckets. 9 is the limit for RcolorBrewer. I bet I can find another, larger color palette but it's fine for now. 
```{r}
binpal <- colorBin("Oranges", sf_311_kitty_summary$calls_bg, 9, pretty = FALSE)
```


```{r}
leaflet(sf_311_kitty_summary) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1, color = ~binpal(calls_bg), popup = ~htmlEscape(calls_bg)) %>% 
  addLegend("bottomright", pal = binpal, values = ~calls_bg,
    title = "Total Calls Per Block",
    opacity = 1
  ) %>% setView(lng =-122.44, lat = 37.758, zoom = 13)
  
```





