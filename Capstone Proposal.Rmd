---
title: "Capstone Proposal"
author: "Max O'Krepki"
date: "2/22/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


It could be fun to try out data.table and see how that works. 
At some point, I probably do want to cut out the code I'm not using, it'll just be cleaner. 
```{r}
library(data.table)
library(dplyr)
library(treemap)
library(sf)
library(readr)
library(leaflet)
library(lubridate)
library(htmltools)
library(ggplot2)
library(ggthemes)
```

Gonna be honest, the importa doesn't seem a whole lot faster than readr. 
Looks like things survived the great reorganization. 
```{r}
sf_311 <- fread("/Users/max/Dropbox/Data Incubator/311_Cases.csv")
sf_311 <- sf_311[,1:20]
```

Hmm, there could also be something interesting going on with the cases that aren't resolved. Probably move this up top when it's all said and done. 
Could also look at time between opening and closing. Just stick to basics for now. 
```{r}
#Let me see if I can't get it to work with the dates as is. 
mdy_hms(sf_311$Opened[1:10], tz = "US/Pacific")

sf_311$Opened <- mdy_hms(sf_311$Opened, tz = "US/Pacific")
sf_311$Closed <- mdy_hms(sf_311$Closed, tz = "US/Pacific")

# Looks like in the closed category there are some NAs, just remove these rows. Relative to the size of the data set, doesn't look like it was too many.  
sf_311 <- sf_311[!is.na(sf_311$Closed),]
```


I'll move up top because I'll use this info to create the separate data frames. 
```{r}
summary_by_category <- sf_311 %>% group_by(Category) %>% summarize("category_summary" = n())
# Let's make a percent of total column to get a better sense of what's going on. 
summary_by_category$pct_of_total <- (summary_by_category$category_summary/sum(summary_by_category$category_summary))*100
summary_by_category <- summary_by_category[order(-summary_by_category$pct_of_total),]
summary_by_category$cumsum <- cumsum(summary_by_category$pct_of_total)

```

Let me just start off with a tree map then go from there. 
```{r}
# Tree map of all
treemap(summary_by_category, index = "Category", vSize = "pct_of_total", type = "index", title = "Percent of Total Calls")

# Tree map of top 10
treemap(summary_by_category[1:10,], index = "Category", vSize = "pct_of_total", type = "index", title = "Percent of Total Calls, Top 10")
```

```{r}
# The dataset is fairly large and I don't need it all, let me filter down to the top four categories. Hmmm, the RData file is still pretty big. Maybe worry about this when I start to make the presentation. 
sf_311 <- sf_311 %>% filter(Category %in% summary_by_category$Category[1:4])
```


Move creation of sf_311_streets here. 
Need to generate sf object from sf_311_streets
```{r}
sf_311_streets <- sf_311 %>% filter(Category == "Street and Sidewalk Cleaning")
# Making the sf object from the data.table
sf_311_streets <- st_as_sf(sf_311_streets, coords = c("Longitude", "Latitude"), crs = "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

# Doing the spatial join to get block group data to each call
sf_311_streets <- st_join(sf_311_streets, ba_bgs["geoid_long"])

# Adding back in lat and long because it's just easier when plotting in leaflet. Although, it takes too long to plot all of the points so I won't end up doing it but this can just stay here for now. 
sf_311_streets <- sf_311_streets %>% left_join(sf_311[,c(1,16,17)], by = "CaseID")

# Filter out coordinates that are clearly wrong. 
sf_311_streets <- sf_311_streets %>% filter(Longitude >= -123 & Longitude <= -122 & Latitude >= 37.7 & Latitude <= 38)
```



At this point, I'll create the data frames for each type then rinse and repeat 4 times. 

Micro goals 
Create data frame for just streets, or use the one I've already got, the calculate time difference between calls and go from there. Even sf_311 streets is just a little too big. It's really just about the output, I don't need to save the data each time. 
```{r}
# This orders the data frame by date.
sf_311_streets <- arrange(sf_311_streets, Opened)

# Create a lag variable between calls. That's the next thing I'm interested in. 
sf_311_streets <- sf_311_streets %>% mutate(time_diff = Opened - lag(Opened), diff_secs = as.numeric(time_diff, units = 'secs'))

# Definitely some outliers. Calculate z scores, remove, then plot again. 
hist(sf_311_streets$diff_secs)
range(sf_311_streets$diff_secs, na.rm = TRUE)

sf_311_streets <- sf_311_streets %>% mutate(diff_z_score = scale(diff_secs))

hist(filter(sf_311_streets, diff_z_score <= 1)$diff_secs)

mean(sf_311_streets$diff_secs, na.rm = TRUE)
median(sf_311_streets$diff_secs, na.rm = TRUE)

mean(filter(sf_311_streets, diff_z_score <= 2)$diff_secs, na.rm = TRUE)
median(filter(sf_311_streets, diff_z_score <= 2)$diff_secs, na.rm = TRUE)
```

Create an economist style histogram
```{r}
barfill <- "#4271AE"
barlines <- "#1F3552"

p7 <- ggplot(sf_311_streets, aes(x = diff_secs)) +
        geom_histogram(aes(y = ..count..), binwidth = 60,
                   colour = barlines, fill = barfill) +
        scale_x_continuous(name = "Time between service call",
                              breaks = seq(0, 1200, 120),
                              limits=c(0, 1200)) +
        scale_y_continuous(name = "Count", labels = scales::comma) +
        ggtitle("Frequency histogram of time \nbetween service calls") +
        theme_economist() +
        theme(legend.position = "bottom", legend.direction = "horizontal",
              legend.box = "horizontal",
              legend.key.size = unit(1, "cm"),
              plot.title = element_text(family="Tahoma"),
              text = element_text(family = "Tahoma", hjust = 0.5),
              axis.title = element_text(size = 12),
              legend.text = element_text(size = 9),
              legend.title=element_text(face = "bold", size = 9))
p7
```


# Geographic hotspots. 
What do I need? Shapefile at the block group level of SF. Should have that somewhere in the thesis work. I tell you what, just upload it here and create an R binary object of it in this working directory. I forgot it's the whole bay area, this will be a round about way to get to SF but it'll work. 

Data's loaded.
```{r}
ba_bgs <- st_read("/Users/max/Dropbox/Stanford Stuff/Classes/PUBLPOL/Y23 Spring 2020/Thesis/Data/sf_bay_clipped/2e262912-207a-4424-9e17-08041688d59c202044-1-130emnv.fypm.shp")

population_data_bgs <- read_csv("~/Dropbox/Stanford Stuff/Classes/PUBLPOL/Y23 Spring 2020/Thesis/Data/population_data_bgs.csv")

ba_bgs <- ba_bgs[,c(6,10)]
ba_bgs$geoid_long <- paste("1500000US", ba_bgs$blkgrpid, sep = "")
ba_bgs <- ba_bgs[,c(2,3)]

ba_bgs <- ba_bgs %>% filter(geoid_long %in% population_data_bgs$id)
```

Map things to make sure they look good. 
```{r}
leaflet(ba_bgs) %>% 
  addProviderTiles(providers$Stamen.Toner) %>% addPolygons()
```

I'll just start with the street and sidewalk cleaning category. Anything I do there abstracts to other categories. 
```{r}
sf_311_streets %>% group_by(`Request Details`) %>% summarize(n())
```



Group calls by geoid. Okay, not sure what happened the first time but it appears to have worked this time. 
```{r}
sf_311_streets_summary <- sf_311_streets %>% group_by(geoid_long) %>% summarize("calls_bg" = n())
plot(sf_311_streets_summary["calls_bg"])

# Join it back to the bgs then plot. 
sf_311_streets_summary_bgs <- ba_bgs %>% left_join(as.data.frame(sf_311_streets_summary)[,1:2], by = "geoid_long")
plot(sf_311_streets_summary_bgs["calls_bg"])
range(sf_311_streets_summary_bgs$calls_bg, na.rm = TRUE) # Looks like some NAs are hanging around. Let me see about getting rid of those. 

# This did the trick. 
sf_311_streets_summary_bgs <- sf_311_streets_summary_bgs[!is.na(sf_311_streets_summary_bgs$calls_bg),]
plot(sf_311_streets_summary_bgs["calls_bg"])
```

Let's add population data and see how things look normalized. You know, I'm not sure I really need the population stuff. This might just get cut. 
```{r}
sf_311_streets_summary_bgs <- sf_311_streets_summary_bgs %>% left_join(population_data_bgs[,c(1,15)], by = c("geoid_long" = "id"))
plot(sf_311_streets_summary_bgs["2017"])

# Replace zeros with 1's.
sf_311_streets_summary_bgs[sf_311_streets_summary_bgs$`2017` == 0,]$`2017` <- 1
range(sf_311_streets_summary_bgs$`2017`)

sf_311_streets_summary_bgs$calls_per_pop <- sf_311_streets_summary_bgs$calls_bg/sf_311_streets_summary_bgs$`2017`

# The problem is that there's the outlier. How about if I remove that. 
plot(sf_311_streets_summary_bgs["calls_per_pop"])
hist(sf_311_streets_summary_bgs$calls_per_pop)

range(sf_311_streets_summary_bgs$calls_per_pop)
hist(filter(sf_311_streets_summary_bgs, calls_per_pop < 10)$calls_per_pop)

sf_311_streets_summary_bgs$z_score <- scale(sf_311_streets_summary_bgs$calls_bg)

range(filter(sf_311_streets_summary_bgs, z_score <= 2)$z_score)
hist(filter(sf_311_streets_summary_bgs, z_score <= 2)$calls_bg)
```

Make a red color palette for it. 
```{r}
pal_red <- colorNumeric(
  palette = "Reds",
  domain = filter(sf_311_streets_summary_bgs, z_score <= 2)$calls_bg
)
```

Let me try a discrete color scheme but with lots of buckets. 9 is the limit for RcolorBrewer. I bet I can find another, larger color palette but it's fine for now. 
```{r}
binpal <- colorBin("Oranges", sf_311_streets_summary_bgs$calls_bg, 9, pretty = FALSE)
```



Funny enough, I'm not seeing much of a spatial pattern especially when adjusted for population. Could be interesting to see about time. With that being said, there's definitely a pattern when considering raw calls per bg.  
```{r}
leaflet(filter(sf_311_streets_summary_bgs, z_score <= 2)) %>% 
  addProviderTiles(providers$CartoDB.Positron) %>% addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,
    color = ~pal_red(calls_bg)) 
```


```{r}
leaflet(sf_311_streets_summary_bgs) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1, color = ~binpal(calls_bg), popup = ~htmlEscape(calls_bg)) %>% 
  addLegend("bottomright", pal = binpal, values = ~calls_bg,
    title = "Total Calls Per Block",
    opacity = 1
  ) %>% setView(lng =-122.44, lat = 37.758, zoom = 13)
  
```





